#!/usr/bin/env python3
"""
Web Scraper and Embedding Tool for Dr. Robert Young's Content
UPDATED: Added intelligent chunking for long articles
"""

# =========================
# Standard library imports
# =========================
import time
import logging
import warnings
from urllib.parse import urljoin

# =========================
# Third-party imports
# =========================
import requests
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer

# =========================
# Local imports
# =========================
from database.db import get_connection

# =========================
# Configuration
# =========================
warnings.filterwarnings("ignore")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

CASE_STUDY_BASE_URL = "https://phoreveryoung.wordpress.com/category/case-studies/"
DR_YOUNG_BASE_URL = "https://drrobertyoung.com/"
HEADERS = {"User-Agent": "Mozilla/5.0 (MultiSiteBot/1.0)"}

MIN_CONTENT_LENGTH = 300
PAGE_DELAY = 1
ARTICLE_DELAY = 2

# =========================
# Embedding model
# =========================
model = SentenceTransformer("all-MiniLM-L6-v2")

# =========================
# âœ… NEW: Chunking function
# =========================
def chunk_text(text, max_words=400, overlap=80):
    """
    Split long text into overlapping chunks
    """
    words = text.split()
    chunks = []

    start = 0
    while start < len(words):
        end = start + max_words
        chunk = " ".join(words[start:end])

        if len(chunk.strip()) > 100:
            chunks.append(chunk)

        start = end - overlap

    return chunks


# =========================
# Content extraction
# =========================
def extract_clean_article_content(soup):
    selectors = [
        "article",
        "div.entry-content",
        "div.elementor-widget-theme-post-content",
        "main"
    ]

    content_root = None
    for sel in selectors:
        content_root = soup.select_one(sel)
        if content_root:
            break

    if not content_root:
        return ""

    elements = content_root.find_all(
        ["p", "h1", "h2", "h3", "h4", "li"],
        recursive=True
    )

    content_parts = []
    for el in elements:
        text = el.get_text(" ", strip=True)

        if len(text) < 30:
            continue

        if any(skip in text.lower() for skip in [
            "share this", "related", "author", "posted on",
            "subscribe", "navigation", "footer",
            "copyright", "privacy policy",
            "terms of service", "cookie", "menu",
            "search", "leave a comment",
            "previous post", "next post",
            "facebook", "twitter", "linkedin"
        ]):
            continue

        content_parts.append(text)

    return "\n".join(content_parts)


# =========================
# Database setup
# =========================
conn = get_connection()
cur = conn.cursor()

# â— UPDATED TABLE (url NOT unique, chunk_index added)
cur.execute("""
CREATE TABLE IF NOT EXISTS dr_young_all_articles (
    id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,
    title TEXT,
    url TEXT,
    chunk_index INT,
    content LONGTEXT,
    embedding LONGTEXT
)
""")
conn.commit()


# =========================
# Case study scraper
# =========================
def scrape_case_studies():
    total_articles = 0
    page_url = CASE_STUDY_BASE_URL

    logger.info("ðŸš€ CASE STUDY SCRAPER STARTED")

    while page_url:
        response = requests.get(page_url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(response.text, "html.parser")

        articles = soup.find_all("article")
        if not articles:
            break

        for art in articles:
            title_link = art.find("a", href=True)
            if not title_link:
                continue

            url = title_link["href"]

            # Skip if already scraped
            cur.execute("SELECT 1 FROM dr_young_all_articles WHERE url=%s", (url,))
            if cur.fetchone():
                continue

            try:
                detail_res = requests.get(url, headers=HEADERS, timeout=10, verify=False)
            except Exception:
                continue

            detail_soup = BeautifulSoup(detail_res.text, "html.parser")
            title_elem = detail_soup.find("h1")
            title = title_elem.get_text(strip=True) if title_elem else ""

            content = extract_clean_article_content(detail_soup)
            if len(content) < MIN_CONTENT_LENGTH:
                continue

            # âœ… CHUNK + EMBED
            chunks = chunk_text(content)

            for idx, chunk in enumerate(chunks):
                embedding = model.encode(chunk).tolist()

                cur.execute("""
                    INSERT INTO dr_young_all_articles
                    (title, url, chunk_index, content, embedding)
                    VALUES (%s, %s, %s, %s, %s)
                """, (title, url, idx, chunk, str(embedding)))

            conn.commit()
            total_articles += 1
            logger.info(f"âœ… INSERTED CASE STUDY (chunks): {title}")
            time.sleep(ARTICLE_DELAY)

        next_link = soup.find("a", class_="next")
        page_url = urljoin(CASE_STUDY_BASE_URL, next_link["href"]) if next_link else None
        time.sleep(PAGE_DELAY)

    return total_articles


# =========================
# Dr Young site scraper
# =========================
def scrape_dr_young_all_categories():
    categories = [
        "blog", "articles", "clean-eating",
        "digestive-health", "womens-health", "corona-virus"
    ]

    total_articles = 0
    logger.info("ðŸš€ DR ROBERT YOUNG SCRAPER STARTED")

    for category in categories:
        category_url = f"https://drrobertyoung.com/{category}/"

        try:
            response = requests.get(category_url, headers=HEADERS, timeout=10, verify=False)
        except Exception:
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        links = soup.find_all("a", href=True)
        post_urls = set()

        for a in links:
            href = a["href"]

            if not href.startswith("https://drrobertyoung.com/"):
                continue

            if any(skip in href for skip in [
                "/wp-content/", "/category/", "/tag/",
                "/page/", "#", "?", "/feed",
                ".jpg", ".png"
            ]):
                continue

            if href.count("-") < 3:
                continue

            post_urls.add(href)

        for url in post_urls:
            cur.execute("SELECT 1 FROM dr_young_all_articles WHERE url=%s", (url,))
            if cur.fetchone():
                continue

            try:
                res = requests.get(url, headers=HEADERS, timeout=10, verify=False)
            except Exception:
                continue

            soup = BeautifulSoup(res.text, "html.parser")
            title_elem = soup.find("h1")
            title = title_elem.get_text(strip=True) if title_elem else url.split("/")[-1]

            content = extract_clean_article_content(soup)
            if len(content) < MIN_CONTENT_LENGTH:
                continue

            # âœ… CHUNK + EMBED
            chunks = chunk_text(content)

            for idx, chunk in enumerate(chunks):
                embedding = model.encode(chunk).tolist()

                cur.execute("""
                    INSERT INTO dr_young_all_articles
                    (title, url, chunk_index, content, embedding)
                    VALUES (%s, %s, %s, %s, %s)
                """, (title, url, idx, chunk, str(embedding)))

            conn.commit()
            total_articles += 1
            logger.info(f"âœ… INSERTED ARTICLE (chunks): {title}")
            time.sleep(ARTICLE_DELAY)

    return total_articles


# =========================
# Main
# =========================
try:
    cs = scrape_case_studies()
    time.sleep(3)
    dr = scrape_dr_young_all_categories()

    logger.info("ðŸŽ‰ SCRAPING COMPLETED")
    logger.info(f"Case Studies: {cs}")
    logger.info(f"Dr Young Articles: {dr}")
    logger.info(f"TOTAL: {cs + dr}")

finally:
    conn.close()
